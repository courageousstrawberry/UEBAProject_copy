{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4a2d142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to set up the notebook, but please don't change it.\n",
    "\n",
    "# These lines import the numpy and datascience modules.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datascience import *\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df1d51da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse a line (in JSON format) and extract into N fields (specified in the array) in csv format\n",
    "def parse_line(line, fields_to_extract):\n",
    "    try:\n",
    "        data = json.loads(line)\n",
    "        parsed_fields = []\n",
    "        \n",
    "        for field in fields_to_extract:\n",
    "            # Get the field value or empty string if not present\n",
    "            value = data.get(field, '')\n",
    "            # Convert to string and replace any commas to avoid CSV issues\n",
    "            parsed_fields.append(str(value).replace(',', ';'))\n",
    "        \n",
    "        # Join the fields with commas\n",
    "        return ','.join(parsed_fields)\n",
    "    except json.JSONDecodeError:\n",
    "        return None  # Return None for invalid JSON lines\n",
    "\n",
    "    \n",
    "    \n",
    "# Function definition to Read the file and write parsed lines to output file\n",
    "def file_convert_csv(input_file, output_file, fields_list):\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "        outfile.write(','.join(fields_list) + '\\n')\n",
    "        for line in infile:\n",
    "            parsed_line = parse_line(line.strip(), fields_to_extract)\n",
    "            if parsed_line is not None:  # Only write valid parsed lines\n",
    "                outfile.write(parsed_line + '\\n')\n",
    "\n",
    "    print(f\"Parsing complete. Output written to {output_file}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92444857",
   "metadata": {},
   "source": [
    "# Step 1: datafile download and format converting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c6e7c1",
   "metadata": {},
   "source": [
    "Step 1 - download the bz2 file from the website, extract into text file, and convert the json format data file into csv format\n",
    "download link (folder) - \n",
    "https://csr.lanl.gov/data-fence/1742781702/j2hoTmkCmL2Rr_G6xYPRIxDEHj0=/unified-host-network-dataset-2017/wls/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d431f027",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\n",
      "  Using cached gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.2)\n",
      "Collecting filelock (from gdown)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.65.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.3.2.post1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2023.5.7)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Using cached gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: filelock, gdown\n",
      "Successfully installed filelock-3.18.0 gdown-5.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://csr.lanl.gov/data-fence/1742781702/j2hoTmkCmL2Rr_G6xYPRIxDEHj0=/unified-host-network-dataset-2017/wls/wls_day-01.bz2\n",
      "To: /home/jovyan/UEBAProject/DataProcess/host/Day2/wls_day-01.bz2\n",
      "100%|██████████| 409M/409M [02:57<00:00, 2.30MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'wls_day-01.bz2'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# modify from Sean's Sample code and download the dialy *.bz2 host data file\n",
    "\n",
    "%pip install gdown\n",
    "import gdown\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# specify the number of day in two-digit\n",
    "no_Day = \"01\"\n",
    "\n",
    "URL = f\"https://csr.lanl.gov/data-fence/1742781702/j2hoTmkCmL2Rr_G6xYPRIxDEHj0=/unified-host-network-dataset-2017/wls/wls_day-{no_Day}.bz2\"\n",
    "\n",
    "gdown.download(URL, quiet = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce48f5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted 'wls_day-01.bz2' to 'wls_day-01'\n"
     ]
    }
   ],
   "source": [
    "#Extract the bz2 file\n",
    "\n",
    "import bz2\n",
    "import shutil\n",
    "\n",
    "def extract_bz2(bz2_filepath, output_filepath):\n",
    "    \"\"\"Extracts a .bz2 file.\n",
    "\n",
    "    Args:\n",
    "        bz2_filepath: Path to the .bz2 file.\n",
    "        output_filepath: Path to save the extracted file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with bz2.BZ2File(bz2_filepath, 'rb') as compressed_file:\n",
    "            with open(output_filepath, 'wb') as uncompressed_file:\n",
    "                shutil.copyfileobj(compressed_file, uncompressed_file)\n",
    "        print(f\"Successfully extracted '{bz2_filepath}' to '{output_filepath}'\")\n",
    "    except FileNotFoundError:\n",
    "         print(f\"Error: The file '{bz2_filepath}' was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Example usage\n",
    "compressed_file_path = f\"wls_day-{no_Day}.bz2\"\n",
    "extracted_file_path = f\"wls_day-{no_Day}\"\n",
    "extract_bz2(compressed_file_path, extracted_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8857980e",
   "metadata": {},
   "source": [
    "# Step 2: Data loading into a table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5308d5",
   "metadata": {},
   "source": [
    "Define the fields we want to extract (total of 21 fields in order)\n",
    "According to the document - https://csr.lanl.gov/data/2017/#citing\n",
    "Need to review the content of the file to see if some attributes should be ignored.\n",
    "   1. 'Time',\n",
    "   2. 'EventID',\n",
    "   3. 'LogHost',\n",
    "   4. 'LogonType',\n",
    "   5. 'LogonTypeDescription',\n",
    "   6. 'UserName',\n",
    "   7. 'DomainName',\n",
    "   8. 'LogonID',\n",
    "   9. 'SubjectUserName',\n",
    "  10. 'SubjectDomainName',\n",
    "  11. 'SubjectLogonID',\n",
    "  12. 'Status',\n",
    "  13. 'Source',\n",
    "  14. 'ServiceName',\n",
    "  15. 'Destination',\n",
    "  16. 'AuthenticationPackage',\n",
    "  17. 'FailureReason',\n",
    "  18. 'ProcessName',\n",
    "  19. 'ProcessID',\n",
    "  20. 'ParentProcessName',\n",
    "  21. 'ParentProcessID'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb875ba",
   "metadata": {},
   "source": [
    "**** Due to process limitation affected by the size of datafile. Some unuse columns are not included in the csv file****\n",
    "\n",
    "Step 2 - load the data file into a table if necessary clean the incorrect format data\n",
    "\n",
    "1. UserName\n",
    "2. EventID\n",
    "3. LogHost\n",
    "4. LogonID\n",
    "5. DomainName\n",
    "6. ParentProcessName\n",
    "7. ParentProcessID\n",
    "8. ProcessName\n",
    "9. Time\n",
    "10. ProcessID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70fb176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_Day = \"01\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c572ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing complete. Output written to wls_day-01.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Define the fields we want to extract (total of 10 fields)\n",
    "fields_to_extract = [\n",
    "    'UserName',\n",
    "    'EventID',\n",
    "    'LogHost',\n",
    "    'LogonID',\n",
    "    'DomainName',\n",
    "    'ParentProcessName',\n",
    "    'ParentProcessID',\n",
    "    'ProcessName',\n",
    "    'Time',\n",
    "    'ProcessID'\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Define the fields we want to extract (total of 21 fields in order)\n",
    "# According to the document - https://csr.lanl.gov/data/2017/#citing\n",
    "fields_to_extract = [\n",
    "    'Time',\n",
    "    'EventID',\n",
    "    'LogHost',\n",
    "    'LogonType',\n",
    "    'LogonTypeDescription',\n",
    "    'UserName',\n",
    "    'DomainName',\n",
    "    'LogonID',\n",
    "    'SubjectUserName',\n",
    "    'SubjectDomainName',\n",
    "    'SubjectLogonID',\n",
    "    'Status',\n",
    "    'Source',\n",
    "    'ServiceName',\n",
    "    'Destination',\n",
    "    'AuthenticationPackage',\n",
    "    'FailureReason',\n",
    "    'ProcessName',\n",
    "    'ProcessID',\n",
    "    'ParentProcessName',\n",
    "    'ParentProcessID'\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "# Input and output file paths\n",
    "input_file = f\"wls_day-{no_Day}\"\n",
    "output_file = f\"wls_day-{no_Day}.csv\"\n",
    "\n",
    "# Call the function to convert the data file into a text file (*.csv) with the array of the data field\n",
    "file_convert_csv(input_file, output_file, fields_to_extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf05aae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddc6eb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_Day = '01'\n",
    "output_file = f\"wls_day-{no_Day}.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c78a79",
   "metadata": {},
   "source": [
    "# divide the datafile into three parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57c89ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV is very large to avoid loading all rows into memory - \n",
    "# divide the large csv file into three parts sequentially\n",
    "\n",
    "import csv\n",
    "import math\n",
    "\n",
    "def split_csv_large(input_file, output_prefix, num_files=6):\n",
    "    # First pass to count rows (excluding header)\n",
    "    with open(input_file, 'r') as infile:\n",
    "        row_count = sum(1 for row in infile) - 1\n",
    "    \n",
    "    rows_per_file = math.ceil(row_count / num_files)\n",
    "    \n",
    "    # Second pass to split the file\n",
    "    with open(input_file, 'r') as infile:\n",
    "        reader = csv.reader(infile)\n",
    "        header = next(reader)\n",
    "        \n",
    "        file_index = 1\n",
    "        current_row = 0\n",
    "        outfile = None\n",
    "        \n",
    "        for row in reader:\n",
    "            if current_row % rows_per_file == 0:\n",
    "                if outfile is not None:\n",
    "                    outfile.close()\n",
    "                outfile = open(f\"{output_prefix}_{file_index}.csv\", 'w', newline='')\n",
    "                writer = csv.writer(outfile)\n",
    "                writer.writerow(header)\n",
    "                file_index += 1\n",
    "            writer.writerow(row)\n",
    "            current_row += 1\n",
    "        \n",
    "        if outfile is not None:\n",
    "            outfile.close()\n",
    "\n",
    "# Example usage:\n",
    "split_csv_large(output_file, f\"{no_Day}S_Part\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9db321bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of lines - 55609150\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m         f\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m \u001b[43mbalanced_random_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mno_Day\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43mR_part\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 36\u001b[0m, in \u001b[0;36mbalanced_random_split\u001b[0;34m(input_file, output_prefix, num_files, random_seed)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mnext\u001b[39m(reader)  \u001b[38;5;66;03m# skip header\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(reader):\n\u001b[0;32m---> 36\u001b[0m         writers[\u001b[43massignments\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m]\u001b[38;5;241m.\u001b[39mwriterow(row)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Close files\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m output_files:\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# approximately equal-sized files with random distribution\n",
    "\n",
    "\n",
    "import csv\n",
    "import random\n",
    "\n",
    "def balanced_random_split(input_file, output_prefix, num_files=3, random_seed=None):\n",
    "    if random_seed is not None:\n",
    "        random.seed(random_seed)\n",
    "    \n",
    "    # First pass to count rows (excluding header)\n",
    "    with open(input_file, 'r') as infile:\n",
    "        reader = csv.reader(infile)\n",
    "        header = next(reader)\n",
    "        row_count = sum(1 for row in infile) - 1\n",
    "        print(f\"total number of lines - {row_count}\")\n",
    "    \n",
    "    # Generate random assignments with balanced counts\n",
    "    assignments = [i % num_files for i in range(row_count)]\n",
    "    random.shuffle(assignments)\n",
    "    \n",
    "    # Initialize output files\n",
    "    output_files = [open(f\"{output_prefix}{i+1}.csv\", 'w', newline='') for i in range(num_files)]\n",
    "    writers = [csv.writer(f) for f in output_files]\n",
    "    \n",
    "    # Write headers\n",
    "    for writer in writers:\n",
    "        writer.writerow(header)\n",
    "    \n",
    "    # Process file and write rows\n",
    "    with open(input_file, 'r') as infile:\n",
    "        reader = csv.reader(infile)\n",
    "        next(reader)  # skip header\n",
    "        \n",
    "        for i, row in enumerate(reader):\n",
    "            writers[assignments[i]].writerow(row)\n",
    "    \n",
    "    # Close files\n",
    "    for f in output_files:\n",
    "        f.close()\n",
    "\n",
    "# Example usage:\n",
    "balanced_random_split(output_file, f\"{no_Day}R_part\", num_files=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c4fb0e",
   "metadata": {},
   "source": [
    "# File access/download from Google Drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9138853a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to access the data file on Google Drive\n",
    "# Define the file path and the number of lines to print\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# The link for the wls_day-02_001.csv (4.3MB) file - \n",
    "# https://drive.google.com/file/d/1t05t6Sn1JrvVfiLOF8fZwZFumzBh0gLI/view?usp=sharing\n",
    "\n",
    "''' \n",
    "FILE_ID = \"1t05t6Sn1JrvVfiLOF8fZwZFumzBh0gLI\"\n",
    "\n",
    "URL = f\"https://drive.google.com/uc?id={FILE_ID}\"\n",
    "\n",
    "df = pd.read_csv(URL)\n",
    "print(df.head())\n",
    "\n",
    "wls_day2_001_url = Table().read_table(URL)\n",
    "wls_day2_001_url.show(10)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e447e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gdown\n",
    "import gdown\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "FILE_ID = \"1UjFNwQltfnvAqCheNAAJ8H_erm0c06Tb\"\n",
    "URL = f\"https://drive.google.com/uc?id={FILE_ID}\"\n",
    "\n",
    "output_file = \"large_file.csv\"\n",
    "gdown.download(URL, output_file, quiet=False)\n",
    "\n",
    "df = dd.read_csv(output_file)\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68a5d64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e034330e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8ad5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the original datafile randomly\n",
    "# Can't perform due to the memory limitation\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def split_csv_into_three(input_file, fileNameHead, output_prefix='output'):\n",
    "    # Read the original CSV file\n",
    "    df = pd.read_csv(input_file)\n",
    "    \n",
    "    # Shuffle the DataFrame rows\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # Calculate split points (approximately 1/3 and 2/3 of the data)\n",
    "    split1 = len(df) // 3\n",
    "    split2 = 2 * split1\n",
    "    \n",
    "    # Split into three parts\n",
    "    df1 = df.iloc[:split1]\n",
    "    df2 = df.iloc[split1:split2]\n",
    "    df3 = df.iloc[split2:]\n",
    "    \n",
    "    # Save to three separate files\n",
    "    df1.to_csv(f'{fileNameHead}{output_prefix}1.csv', index=False)\n",
    "    df2.to_csv(f'{fileNameHead}{output_prefix}2.csv', index=False)\n",
    "    df3.to_csv(f'{fileNameHead}{output_prefix}3.csv', index=False)\n",
    "    \n",
    "    print(f\"Successfully split {input_file} into three files:\")\n",
    "    print(f\"- {fileNameHead}{output_prefix}1.csv ({len(df1)} rows)\")\n",
    "    print(f\"- {fileNameHead}{output_prefix}2.csv ({len(df2)} rows)\")\n",
    "    print(f\"- {fileNameHead}{output_prefix}3.csv ({len(df3)} rows)\")\n",
    "\n",
    "# Usage example\n",
    "\n",
    "# Input and output file paths\n",
    "input_file = 'wls_day-01.csv'\n",
    "\n",
    "#output_file1 = 'wls_day-02_Part1.csv'  \n",
    "#output_file2 = 'wls_day-02_Part2.csv'\n",
    "#output_file3 = 'wls_day-02_Part3.csv'\n",
    "\n",
    "split_csv_into_three(input_file,'wls_day-01', 'Part')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4165960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the datafile into three parts sequentially\n",
    "# Fail to perform on the cloud as well but can be complete at Laura's Mac\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the original CSV file\n",
    "df = pd.read_csv('wls_day-01.csv')\n",
    "\n",
    "# Calculate the size of each part\n",
    "part_size = len(df) // 3\n",
    "\n",
    "# Split into three parts\n",
    "part1 = df.iloc[:part_size]\n",
    "part2 = df.iloc[part_size:2*part_size]\n",
    "part3 = df.iloc[2*part_size:]\n",
    "\n",
    "# Save to new CSV files\n",
    "part1.to_csv('wls_day-01part1.csv', index=False)\n",
    "part2.to_csv('wls_day-01part2.csv', index=False)\n",
    "part3.to_csv('wls_day-01part3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d6dd89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get info using pandas\n",
    "# still failed\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('wls_day-01.csv')\n",
    "print(df.shape)\n",
    "print(df.describe())\n",
    "print(df.dtypes)\n",
    "print(df.head())\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616cba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get info using dataScience package\n",
    "wls_day1 = Table().read_table('wls_day-01_10.csv')\n",
    "print(wls_day1.num_rows)\n",
    "wls_day1.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
